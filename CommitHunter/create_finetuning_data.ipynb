{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrnHXaPDFzRQghX6QL4Pix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShantKhatri/aqa-triage-data/blob/create_finetuning_data/CommitHunter/create_finetuning_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yX4JPZw3737",
        "outputId": "ed0729a6-b94b-452b-e251-d01dd2d8ad83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'openj9'...\n",
            "remote: Enumerating objects: 293519, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 293519 (delta 116), reused 78 (delta 78), pack-reused 293348 (from 3)\u001b[K\n",
            "Receiving objects: 100% (293519/293519), 193.10 MiB | 14.12 MiB/s, done.\n",
            "Resolving deltas: 100% (222549/222549), done.\n",
            "Updating files: 100% (10320/10320), done.\n",
            "/content/openj9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!git clone https://github.com/eclipse-openj9/openj9.git\n",
        "%cd openj9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "def get_commit_message(sha):\n",
        "    \"\"\"Fetches the subject line of a commit message.\"\"\"\n",
        "    try:\n",
        "        return subprocess.check_output(\n",
        "            [\"git\", \"show\", \"-s\", \"--format=%s\", sha]\n",
        "        ).decode().strip()\n",
        "    except subprocess.CalledProcessError:\n",
        "        return \"[Could not fetch commit message]\"\n",
        "\n",
        "def generate_analysis_prompt(good_sha, bad_sha, failure_logs):\n",
        "    \"\"\"\n",
        "    Generates the detailed prompt string by querying the git repository for commit data\n",
        "    between the good and bad SHAs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        commit_list = subprocess.check_output(\n",
        "            [\"git\", \"rev-list\", \"--reverse\", \"--no-merges\", f\"{good_sha}..{bad_sha}\"]\n",
        "        ).decode().strip().splitlines()\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Warning: Could not find commits between {good_sha} and {bad_sha}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    prompt_lines = [\n",
        "        \"==== INPUT FOR COMMIT HUNTER (SHORT VERSION) ====\\n\",\n",
        "        f\"Good Build SHA: {good_sha}\",\n",
        "        f\"Bad Build SHA:  {bad_sha}\\n\",\n",
        "        f\"Analysis of Commits Between {good_sha} and {bad_sha}\",\n",
        "        \"=\" * 60 + \"\\n\"\n",
        "    ]\n",
        "\n",
        "    for i, sha in enumerate(commit_list):\n",
        "        commit_details_format = \"commit:%H%nCommit message: %s%nAuthor Name: %an\"\n",
        "        commit_details = subprocess.check_output(\n",
        "            [\"git\", \"show\", \"-s\", f\"--format={commit_details_format}\", sha]\n",
        "        ).decode().strip()\n",
        "        prompt_lines.append(commit_details)\n",
        "\n",
        "        files_changed_raw = subprocess.check_output(\n",
        "            [\"git\", \"diff-tree\", \"--no-commit-id\", \"--name-only\", \"-r\", sha]\n",
        "        ).decode().strip()\n",
        "        prompt_lines.append(\"Files Changed:\")\n",
        "        for file_line in files_changed_raw.splitlines():\n",
        "            prompt_lines.append(f\"  - {file_line}\")\n",
        "\n",
        "        diff_content = subprocess.check_output(\n",
        "            [\"git\", \"show\", sha, \"--unified=5\"]\n",
        "        ).decode()\n",
        "\n",
        "        diff_start_index = diff_content.find(\"diff --git\")\n",
        "        if diff_start_index != -1:\n",
        "            clean_diff = diff_content[diff_start_index:]\n",
        "            prompt_lines.append(\"Content changed(diff):\")\n",
        "            prompt_lines.append(clean_diff[:4000])\n",
        "\n",
        "        prompt_lines.append(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "    prompt_lines.extend([\n",
        "        \"=== Failed Test Context ===\\n\",\n",
        "        textwrap.dedent(failure_logs),\n",
        "        \"\\n\",\n",
        "        \"=== Prompt ===\\n\",\n",
        "        textwrap.dedent(\"\"\"\n",
        "            Think step-by-step and explain your reasoning before giving the final answer.\n",
        "            Based on the commits, changed files, content changed, test failure and stack trace,\n",
        "            which commit is most likely responsible for the regression?\n",
        "            Do major focus on the changed files and the contentchanged with respect to the failure trace we have, as commit message may some times\n",
        "            not related as it totally dependson the developer, also we have some merge message so, better to focus on the content and files changed.\n",
        "            TASK: Analyze the commits above and identify which specific commit is most likely responsible for causing a build failure in OpenJ9.\n",
        "\n",
        "            RESPONSE FORMAT:\n",
        "            1. Most Suspicious Commit: [commit hash and message(Important)]\n",
        "            2. Risk Level: [High/Medium/Low]\n",
        "            3. Reasoning: [detailed explanation]\n",
        "            4. Recommendation: [what to investigate next]\n",
        "\n",
        "            List about 2-3 suspected commits.\n",
        "        \"\"\")\n",
        "    ])\n",
        "\n",
        "    return \"\\n\".join(prompt_lines)\n",
        "\n",
        "def generate_ideal_completion(culprit_sha, failure_logs, reasoning):\n",
        "    \"\"\"\n",
        "    Generates the ideal 'assistant' response based on the ground truth culprit SHA\n",
        "    and the reasoning provided.\n",
        "    \"\"\"\n",
        "    culprit_message = get_commit_message(culprit_sha)\n",
        "\n",
        "    # Use the reasoning from the JSON if available, otherwise generate a default.\n",
        "    if reasoning and reasoning.strip():\n",
        "        # Clean up the provided reasoning for better formatting\n",
        "        lines = reasoning.strip().split('\\n')\n",
        "        cleaned_reasoning = \"\\n\".join(line.strip() for line in lines)\n",
        "    else:\n",
        "        cleaned_reasoning = \"A detailed analysis of the culprit commit is required to determine the root cause. The failure logs should be cross-referenced with the code changes in the commit.\"\n",
        "\n",
        "    completion = textwrap.dedent(f\"\"\"\n",
        "        1. Most Suspicious Commit: {culprit_sha} (\"{culprit_message}\")\n",
        "        2. Risk Level: High\n",
        "        3. Reasoning: {cleaned_reasoning}\n",
        "    \"\"\").strip()\n",
        "\n",
        "    return completion\n",
        "\n",
        "def create_finetuning_data():\n",
        "    \"\"\"\n",
        "    Main function to generate the finetuning JSONL file.\n",
        "    Loads failure data from processed_prs.json\n",
        "    \"\"\"\n",
        "    input_filename = \"processed_prs.json\"\n",
        "    output_filename = \"finetuning_data.jsonl\"\n",
        "\n",
        "    print(f\"Loading failure data from '{input_filename}'...\")\n",
        "    with open(input_filename, \"r\") as f_in:\n",
        "        failure_data_list = json.load(f_in)\n",
        "\n",
        "    print(f\"Starting fine-tuning data generation with {len(failure_data_list)} records...\")\n",
        "\n",
        "    with open(output_filename, \"w\") as f_out:\n",
        "        for i, data in enumerate(failure_data_list):\n",
        "            print(f\"Processing record {i+1}/{len(failure_data_list)} for PR {data['pr']}...\")\n",
        "\n",
        "            prompt_content = generate_analysis_prompt(\n",
        "                data['goodSHA'], data['badSHA'], data['failure_logs']\n",
        "            )\n",
        "            if prompt_content is None:\n",
        "                continue\n",
        "\n",
        "            completion_content = generate_ideal_completion(\n",
        "                data['culprit_SHA'], data['failure_logs'], data.get('reasoning')\n",
        "            )\n",
        "\n",
        "            fine_tuning_record = {\n",
        "                \"prompt\": prompt_content,\n",
        "                \"completion\": completion_content\n",
        "            }\n",
        "\n",
        "            f_out.write(json.dumps(fine_tuning_record) + \"\\n\")\n",
        "\n",
        "    print(f\"\\Fine-tuning data successfully written to '{output_filename}'\")\n",
        "\n",
        "\n",
        "create_finetuning_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEzUJI9V3_if",
        "outputId": "ebc4361a-1101-4e24-e533-d4e719b9acfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting fine-tuning data generation...\n",
            "Processing record 1/1 for PR 22185...\n",
            "\n",
            "âœ… Fine-tuning data successfully written to 'finetuning_data.jsonl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30irGNSG40W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below script is to find the good sha and bad sha from the culprit commit.\n",
        "## This will return the last commit of the previous day and the last commit of the current day"
      ],
      "metadata": {
        "id": "WXXNVHge9Ksm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "target_sha = \"7ee010c7af9d7a9415a3aace2ec2bcae0af808c9\"\n",
        "\n",
        "repo_url = \"https://github.com/eclipse-openj9/openj9.git\"\n",
        "repo_dir = \"openj9\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "  print(f\"Cloning repository: {repo_url}...\")\n",
        "  !git clone --quiet {repo_url}\n",
        "else:\n",
        "  print(f\"Repository '{repo_dir}' already exists.\")\n",
        "\n",
        "def run_command(command, working_dir, timezone=\"America/New_York\"):\n",
        "  \"\"\"Runs a shell command and returns its output or error.\"\"\"\n",
        "  env = os.environ.copy()\n",
        "  env['TZ'] = timezone\n",
        "\n",
        "  result = subprocess.run(\n",
        "      command,\n",
        "      cwd=working_dir,\n",
        "      capture_output=True,\n",
        "      text=True,\n",
        "      shell=True,\n",
        "      env=env\n",
        "  )\n",
        "\n",
        "  if result.returncode != 0:\n",
        "    print(f\"Error running command: {command}\")\n",
        "    return None, result.stderr.strip()\n",
        "\n",
        "  return result.stdout.strip(), None\n",
        "\n",
        "print(f\"\\nGetting date for target SHA: {target_sha[:10]}...\")\n",
        "date_command = f\"git show -s --format=%cs {target_sha}\"\n",
        "commit_date, error = run_command(date_command, repo_dir)\n",
        "\n",
        "if error:\n",
        "  print(f\"Error: {error}\")\n",
        "else:\n",
        "  print(f\"Target commit date (NY Time): {commit_date}\")\n",
        "\n",
        "  print(\"\\nSearching for the last commit of the PREVIOUS day...\")\n",
        "  # ADDED --pretty=fuller to show both dates\n",
        "  prev_day_cmd = f\"git log --all --pretty=fuller --before='{commit_date} 00:00:00' -n 1\"\n",
        "  prev_day_info, prev_error = run_command(prev_day_cmd, repo_dir)\n",
        "\n",
        "  if prev_error:\n",
        "    print(f\"Error: {prev_error}\")\n",
        "  elif prev_day_info:\n",
        "    print(\"\\n--- âœ… Last Commit of PREVIOUS Day ---\")\n",
        "    print(prev_day_info)\n",
        "    print(\"---------------------------------------\")\n",
        "  else:\n",
        "    print(\"No commits found for the previous day.\")\n",
        "\n",
        "  print(\"\\nSearching for the last commit of the SAME day...\")\n",
        "  same_day_cmd = f\"git log --all --pretty=fuller --since='{commit_date} 00:00:00' --until='{commit_date} 23:59:59' -n 1\"\n",
        "  same_day_info, same_error = run_command(same_day_cmd, repo_dir)\n",
        "\n",
        "  if same_error:\n",
        "    print(f\"Error: {same_error}\")\n",
        "  elif same_day_info:\n",
        "    print(\"\\n--- âœ… Last Commit of SAME Day ---\")\n",
        "    print(same_day_info)\n",
        "    print(\"-----------------------------------\")\n",
        "  else:\n",
        "    print(\"No commits found for the same day.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EIn0UV79CQf",
        "outputId": "c795dc8a-5d47-42d1-ef52-8d0eab4cf398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository: https://github.com/eclipse-openj9/openj9.git...\n",
            "\n",
            "Getting date for target SHA: 7ee010c7af...\n",
            "Target commit date (NY Time): 2025-02-12\n",
            "\n",
            "Searching for the last commit of the PREVIOUS day...\n",
            "\n",
            "--- âœ… Last Commit of PREVIOUS Day ---\n",
            "commit 07b21da3c485c5c433a8122e94873b17e5240c20\n",
            "Author:     Daryl Maier <maier@ca.ibm.com>\n",
            "AuthorDate: Tue Feb 11 22:23:31 2025 -0500\n",
            "Commit:     Daryl Maier <maier@ca.ibm.com>\n",
            "CommitDate: Tue Feb 11 22:23:31 2025 -0500\n",
            "\n",
            "    Remove unnecessary includes of CS2 headers\n",
            "    \n",
            "    Signed-off-by: Daryl Maier <maier@ca.ibm.com>\n",
            "---------------------------------------\n",
            "\n",
            "Searching for the last commit of the SAME day...\n",
            "\n",
            "--- âœ… Last Commit of SAME Day ---\n",
            "commit 8c3feb95d0d7b2fc2cfcd2e9d87bdbbcdb18f358\n",
            "Merge: 5d76f28aae 3d1cf31be7\n",
            "Author:     Keith W. Campbell <keithc@ca.ibm.com>\n",
            "AuthorDate: Wed Feb 12 17:54:38 2025 -0500\n",
            "Commit:     GitHub <noreply@github.com>\n",
            "CommitDate: Wed Feb 12 17:54:38 2025 -0500\n",
            "\n",
            "    Merge pull request #21093 from theresa-m/fix_21055\n",
            "    \n",
            "    New unsafe method stubs for ValueTypes\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_wz8KjDuF6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}